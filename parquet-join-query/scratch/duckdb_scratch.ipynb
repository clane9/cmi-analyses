{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r002.ib.bridges2.psc.edu\n"
     ]
    }
   ],
   "source": [
    "! hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import duckdb\n",
    "from pyarrow import dataset as pads\n",
    "from pyarrow import parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"data\"\n",
    "nrows = 10000\n",
    "nelem = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mpq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParquetDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpath_or_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfilesystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msplit_row_groups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvalidate_schema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmetadata_nthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mread_dictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_legacy_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpre_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcoerce_int96_timestamp_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mthrift_string_size_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mthrift_container_size_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Encapsulates details of reading a complete Parquet dataset possibly\n",
      "consisting of multiple files and partitions in subdirectories.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "path_or_paths : str or List[str]\n",
      "    A directory name, single file name, or list of file names.\n",
      "filesystem : FileSystem, default None\n",
      "    If nothing passed, will be inferred based on path.\n",
      "    Path will try to be found in the local on-disk filesystem otherwise\n",
      "    it will be parsed as an URI to determine the filesystem.\n",
      "metadata : pyarrow.parquet.FileMetaData\n",
      "    Use metadata obtained elsewhere to validate file schemas.\n",
      "schema : pyarrow.parquet.Schema\n",
      "    Use schema obtained elsewhere to validate file schemas. Alternative to\n",
      "    metadata parameter.\n",
      "split_row_groups : bool, default False\n",
      "    Divide files into pieces for each row group in the file.\n",
      "validate_schema : bool, default True\n",
      "    Check that individual file schemas are all the same / compatible.\n",
      "filters : List[Tuple] or List[List[Tuple]] or None (default)\n",
      "    Rows which do not match the filter predicate will be removed from scanned\n",
      "    data. Partition keys embedded in a nested directory structure will be\n",
      "    exploited to avoid loading files at all if they contain no matching rows.\n",
      "    If `use_legacy_dataset` is True, filters can only reference partition\n",
      "    keys and only a hive-style directory structure is supported. When\n",
      "    setting `use_legacy_dataset` to False, also within-file level filtering\n",
      "    and different partitioning schemes are supported.\n",
      "\n",
      "    Predicates are expressed in disjunctive normal form (DNF),\n",
      "    like ``[[('x', '=', 0), ...], ...]``. DNF allows arbitrary boolean logical\n",
      "    combinations of single column predicates. The innermost tuples each\n",
      "    describe a single column predicate. The list of inner predicates is\n",
      "    interpreted as a conjunction (AND), forming a more selective and multiple\n",
      "    column predicate. Finally, the most outer list combines these filters as a\n",
      "    disjunction (OR).\n",
      "\n",
      "    Predicates may also be passed as List[Tuple]. This form is interpreted\n",
      "    as a single conjunction. To express OR in predicates, one must\n",
      "    use the (preferred) List[List[Tuple]] notation.\n",
      "\n",
      "    Each tuple has format: (``key``, ``op``, ``value``) and compares the\n",
      "    ``key`` with the ``value``.\n",
      "    The supported ``op`` are:  ``=`` or ``==``, ``!=``, ``<``, ``>``, ``<=``,\n",
      "    ``>=``, ``in`` and ``not in``. If the ``op`` is ``in`` or ``not in``, the\n",
      "    ``value`` must be a collection such as a ``list``, a ``set`` or a\n",
      "    ``tuple``.\n",
      "\n",
      "    Examples:\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        ('x', '=', 0)\n",
      "        ('y', 'in', ['a', 'b', 'c'])\n",
      "        ('z', 'not in', {'a','b'})\n",
      "\n",
      "    \n",
      "metadata_nthreads : int, default 1\n",
      "    How many threads to allow the thread pool which is used to read the\n",
      "    dataset metadata. Increasing this is helpful to read partitioned\n",
      "    datasets.\n",
      "read_dictionary : list, default None\n",
      "    List of names or column paths (for nested types) to read directly\n",
      "    as DictionaryArray. Only supported for BYTE_ARRAY storage. To read\n",
      "    a flat column as dictionary-encoded pass the column name. For\n",
      "    nested types, you must pass the full column \"path\", which could be\n",
      "    something like level1.level2.list.item. Refer to the Parquet\n",
      "    file's schema to obtain the paths.\n",
      "memory_map : bool, default False\n",
      "    If the source is a file path, use a memory map to read file, which can\n",
      "    improve performance in some environments.\n",
      "buffer_size : int, default 0\n",
      "    If positive, perform read buffering when deserializing individual\n",
      "    column chunks. Otherwise IO calls are unbuffered.\n",
      "partitioning : pyarrow.dataset.Partitioning or str or list of str, default \"hive\"\n",
      "    The partitioning scheme for a partitioned dataset. The default of \"hive\"\n",
      "    assumes directory names with key=value pairs like \"/year=2009/month=11\".\n",
      "    In addition, a scheme like \"/2009/11\" is also supported, in which case\n",
      "    you need to specify the field names or a full schema. See the\n",
      "    ``pyarrow.dataset.partitioning()`` function for more details.\n",
      "use_legacy_dataset : bool, default True\n",
      "    Set to False to enable the new code path (using the\n",
      "    new Arrow Dataset API). Among other things, this allows to pass\n",
      "    `filters` for all columns and not only the partition keys, enables\n",
      "    different partitioning schemes, etc.\n",
      "pre_buffer : bool, default True\n",
      "    Coalesce and issue file reads in parallel to improve performance on\n",
      "    high-latency filesystems (e.g. S3). If True, Arrow will use a\n",
      "    background I/O thread pool. This option is only supported for\n",
      "    use_legacy_dataset=False. If using a filesystem layer that itself\n",
      "    performs readahead (e.g. fsspec's S3FS), disable readahead for best\n",
      "    results.\n",
      "coerce_int96_timestamp_unit : str, default None.\n",
      "    Cast timestamps that are stored in INT96 format to a particular resolution\n",
      "    (e.g. 'ms'). Setting to None is equivalent to 'ns' and therefore INT96\n",
      "    timestamps will be inferred as timestamps in nanoseconds.\n",
      "thrift_string_size_limit : int, default None\n",
      "    If not None, override the maximum total string size allocated\n",
      "    when decoding Thrift structures. The default limit should be\n",
      "    sufficient for most Parquet files.\n",
      "thrift_container_size_limit : int, default None\n",
      "    If not None, override the maximum total size of containers allocated\n",
      "    when decoding Thrift structures. The default limit should be\n",
      "    sufficient for most Parquet files.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Generate an example PyArrow Table and write it to a partitioned dataset:\n",
      "\n",
      ">>> import pyarrow as pa\n",
      ">>> table = pa.table({'year': [2020, 2022, 2021, 2022, 2019, 2021],\n",
      "...                   'n_legs': [2, 2, 4, 4, 5, 100],\n",
      "...                   'animal': [\"Flamingo\", \"Parrot\", \"Dog\", \"Horse\",\n",
      "...                              \"Brittle stars\", \"Centipede\"]})\n",
      "\n",
      ">>> import pyarrow.parquet as pq\n",
      ">>> pq.write_to_dataset(table, root_path='dataset_name',\n",
      "...                     partition_cols=['year'],\n",
      "...                     use_legacy_dataset=False)\n",
      "\n",
      "create a ParquetDataset object from the dataset source:\n",
      "\n",
      ">>> dataset = pq.ParquetDataset('dataset_name/', use_legacy_dataset=False)\n",
      "\n",
      "and read the data:\n",
      "\n",
      ">>> dataset.read().to_pandas()\n",
      "   n_legs         animal  year\n",
      "0       5  Brittle stars  2019\n",
      "1       2       Flamingo  2020\n",
      "2       4            Dog  2021\n",
      "3     100      Centipede  2021\n",
      "4       2         Parrot  2022\n",
      "5       4          Horse  2022\n",
      "\n",
      "create a ParquetDataset object with filter:\n",
      "\n",
      ">>> dataset = pq.ParquetDataset('dataset_name/', use_legacy_dataset=False,\n",
      "...                             filters=[('n_legs','=',4)])\n",
      ">>> dataset.read().to_pandas()\n",
      "   n_legs animal  year\n",
      "0       4    Dog  2021\n",
      "1       4  Horse  2022\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/python39/lib/python3.9/site-packages/pyarrow/parquet/core.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "pq.ParquetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = Path(root) / f\"nrows-{nrows}_nelem-{nelem}\"\n",
    "attrib = pads.dataset(str(datadir / \"attrib.parquet\"))\n",
    "arrays = pads.dataset(list(sorted(glob(str(datadir / \"arrays-*.parquet\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = con.query(\"SELECT * from attrib where attrib.attrib < 0.1\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attrib</th>\n",
       "      <th>__index_level_0__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029850</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.064889</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037918</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050138</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.063490</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     attrib  __index_level_0__\n",
       "0  0.029850                 16\n",
       "1  0.064889                 21\n",
       "2  0.037918                 22\n",
       "3  0.050138                 28\n",
       "4  0.063490                 41"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"SELECT \n",
    "    attrib.__index_level_0__, arrays.__index_level_0__, attrib.attrib, arrays.array\n",
    "from attrib\n",
    "left join arrays\n",
    "    on attrib.__index_level_0__ = arrays.__index_level_0__\n",
    "where attrib.attrib < 0.1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "\n",
    "sql = f\"\"\"\n",
    "with filtered as (\n",
    "    select * from attrib where attrib.attrib < {threshold:.6f}\n",
    ")\n",
    "select\n",
    "    filtered.__index_level_0__, arrays.__index_level_0__, filtered.attrib, arrays.array\n",
    "from filtered \n",
    "left join arrays\n",
    "    on filtered.__index_level_0__ = arrays.__index_level_0__\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = con.query(sql).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>__index_level_0___2</th>\n",
       "      <th>attrib</th>\n",
       "      <th>array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.029850</td>\n",
       "      <td>[-0.3740611486267887, 0.701022828054255, -1.91...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0.064889</td>\n",
       "      <td>[-0.12444886848162703, 0.7523795492775105, 1.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>0.050138</td>\n",
       "      <td>[-2.1507090466901246, -0.5456877844872032, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>0.063490</td>\n",
       "      <td>[-0.05590238127902171, 0.22616237487552604, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>[0.5239562476585321, 1.114994197727871, 0.2957...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   __index_level_0__  __index_level_0___2    attrib  \\\n",
       "0                 16                   16  0.029850   \n",
       "1                 21                   21  0.064889   \n",
       "2                 28                   28  0.050138   \n",
       "3                 41                   41  0.063490   \n",
       "4                 42                   42  0.064209   \n",
       "\n",
       "                                               array  \n",
       "0  [-0.3740611486267887, 0.701022828054255, -1.91...  \n",
       "1  [-0.12444886848162703, 0.7523795492775105, 1.7...  \n",
       "2  [-2.1507090466901246, -0.5456877844872032, -0....  \n",
       "3  [-0.05590238127902171, 0.22616237487552604, -0...  \n",
       "4  [0.5239562476585321, 1.114994197727871, 0.2957...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54178399f08d9e4f4a60ab23f4a210a71377e48c9bb1378553c53e5ab291a7fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
